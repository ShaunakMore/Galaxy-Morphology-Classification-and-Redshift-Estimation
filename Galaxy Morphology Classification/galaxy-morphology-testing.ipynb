{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cacaa00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Setting up dataset with labels extracted from filenames\n",
      "Sample 0: Filename = image_0_0.png, Extracted Label = 0\n",
      "Sample 1: Filename = image_10000_6.png, Extracted Label = 6\n",
      "Sample 2: Filename = image_10001_6.png, Extracted Label = 6\n",
      "Sample 3: Filename = image_10002_6.png, Extracted Label = 6\n",
      "Sample 4: Filename = image_10003_6.png, Extracted Label = 6\n",
      "Sample 5: Filename = image_10004_6.png, Extracted Label = 6\n",
      "Sample 6: Filename = image_10005_6.png, Extracted Label = 6\n",
      "Sample 7: Filename = image_10006_6.png, Extracted Label = 6\n",
      "Sample 8: Filename = image_10007_6.png, Extracted Label = 6\n",
      "Sample 9: Filename = image_10008_6.png, Extracted Label = 6\n",
      "Dataset split into 14189 training and 3547 validation samples\n",
      "Model loaded successfully from best_model.pth\n",
      "\n",
      "Evaluating model on training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making Predictions: 100%|██████████| 444/444 [01:26<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "VALIDATION ACCURACY: 90.77%\n",
      "Correct predictions: 12879/14189\n",
      "==================================================\n",
      "\n",
      "Model Accuracy: 90.77%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.70      0.75       888\n",
      "           1       0.95      0.97      0.96      1515\n",
      "           2       0.94      0.97      0.95      2102\n",
      "           3       0.93      0.97      0.95      1613\n",
      "           4       0.76      1.00      0.87       263\n",
      "           5       0.86      0.94      0.89      1624\n",
      "           6       0.86      0.86      0.86      1484\n",
      "           7       0.89      0.74      0.81      2069\n",
      "           8       0.95      0.99      0.97      1134\n",
      "           9       0.97      0.97      0.97      1497\n",
      "\n",
      "    accuracy                           0.91     14189\n",
      "   macro avg       0.89      0.91      0.90     14189\n",
      "weighted avg       0.91      0.91      0.91     14189\n",
      "\n",
      "Confusion matrix saved to 'confusion_matrix_final.png'\n",
      "\n",
      "Per-Class Accuracy:\n",
      "Class 0: 70.50% (626/888)\n",
      "Class 1: 96.96% (1469/1515)\n",
      "Class 2: 96.86% (2036/2102)\n",
      "Class 3: 97.33% (1570/1613)\n",
      "Class 4: 100.00% (263/263)\n",
      "Class 5: 93.60% (1520/1624)\n",
      "Class 6: 86.32% (1281/1484)\n",
      "Class 7: 74.48% (1541/2069)\n",
      "Class 8: 98.50% (1117/1134)\n",
      "Class 9: 97.26% (1456/1497)\n",
      "\n",
      "Saved 1310 incorrect predictions to 'incorrect_predictions.txt'\n",
      "\n",
      "Evaluating model on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making Predictions: 100%|██████████| 111/111 [00:22<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "VALIDATION ACCURACY: 90.19%\n",
      "Correct predictions: 3199/3547\n",
      "==================================================\n",
      "\n",
      "Model Accuracy: 90.19%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.69      0.72       193\n",
      "           1       0.94      0.96      0.95       338\n",
      "           2       0.94      0.97      0.95       543\n",
      "           3       0.94      0.97      0.96       414\n",
      "           4       0.77      1.00      0.87        71\n",
      "           5       0.87      0.90      0.88       419\n",
      "           6       0.82      0.90      0.86       345\n",
      "           7       0.89      0.73      0.80       559\n",
      "           8       0.95      0.98      0.97       289\n",
      "           9       0.96      0.97      0.96       376\n",
      "\n",
      "    accuracy                           0.90      3547\n",
      "   macro avg       0.88      0.91      0.89      3547\n",
      "weighted avg       0.90      0.90      0.90      3547\n",
      "\n",
      "Confusion matrix saved to 'confusion_matrix_final.png'\n",
      "\n",
      "Per-Class Accuracy:\n",
      "Class 0: 68.91% (133/193)\n",
      "Class 1: 95.86% (324/338)\n",
      "Class 2: 96.87% (526/543)\n",
      "Class 3: 97.10% (402/414)\n",
      "Class 4: 100.00% (71/71)\n",
      "Class 5: 89.98% (377/419)\n",
      "Class 6: 89.86% (310/345)\n",
      "Class 7: 73.17% (409/559)\n",
      "Class 8: 97.92% (283/289)\n",
      "Class 9: 96.81% (364/376)\n",
      "\n",
      "Saved 348 incorrect predictions to 'incorrect_predictions.txt'\n",
      "Distribution saved to 'training_set_predictions.png'\n",
      "\n",
      "Training Set Predictions Summary:\n",
      "Class 0: 779 images (5.49%)\n",
      "Class 1: 1542 images (10.87%)\n",
      "Class 2: 2165 images (15.26%)\n",
      "Class 3: 1683 images (11.86%)\n",
      "Class 4: 344 images (2.42%)\n",
      "Class 5: 1773 images (12.50%)\n",
      "Class 6: 1491 images (10.51%)\n",
      "Class 7: 1728 images (12.18%)\n",
      "Class 8: 1180 images (8.32%)\n",
      "Class 9: 1504 images (10.60%)\n",
      "Distribution saved to 'training_set_true_labels.png'\n",
      "\n",
      "Training Set True Labels Summary:\n",
      "Class 0: 888 images (6.26%)\n",
      "Class 1: 1515 images (10.68%)\n",
      "Class 2: 2102 images (14.81%)\n",
      "Class 3: 1613 images (11.37%)\n",
      "Class 4: 263 images (1.85%)\n",
      "Class 5: 1624 images (11.45%)\n",
      "Class 6: 1484 images (10.46%)\n",
      "Class 7: 2069 images (14.58%)\n",
      "Class 8: 1134 images (7.99%)\n",
      "Class 9: 1497 images (10.55%)\n",
      "Distribution saved to 'validation_set_predictions.png'\n",
      "\n",
      "Validation Set Predictions Summary:\n",
      "Class 0: 177 images (4.99%)\n",
      "Class 1: 343 images (9.67%)\n",
      "Class 2: 560 images (15.79%)\n",
      "Class 3: 427 images (12.04%)\n",
      "Class 4: 92 images (2.59%)\n",
      "Class 5: 434 images (12.24%)\n",
      "Class 6: 378 images (10.66%)\n",
      "Class 7: 458 images (12.91%)\n",
      "Class 8: 297 images (8.37%)\n",
      "Class 9: 381 images (10.74%)\n",
      "Distribution saved to 'validation_set_true_labels.png'\n",
      "\n",
      "Validation Set True Labels Summary:\n",
      "Class 0: 193 images (5.44%)\n",
      "Class 1: 338 images (9.53%)\n",
      "Class 2: 543 images (15.31%)\n",
      "Class 3: 414 images (11.67%)\n",
      "Class 4: 71 images (2.00%)\n",
      "Class 5: 419 images (11.81%)\n",
      "Class 6: 345 images (9.73%)\n",
      "Class 7: 559 images (15.76%)\n",
      "Class 8: 289 images (8.15%)\n",
      "Class 9: 376 images (10.60%)\n",
      "\n",
      "Generating test cases from validation set...\n",
      "\n",
      "============================================================\n",
      "GENERATING TEST CASES AND RESULTS SECTION\n",
      "============================================================\n",
      "Processing test case 1/15...\n",
      "Processing test case 2/15...\n",
      "Processing test case 3/15...\n",
      "Processing test case 4/15...\n",
      "Processing test case 5/15...\n",
      "Processing test case 6/15...\n",
      "Processing test case 7/15...\n",
      "Processing test case 8/15...\n",
      "Processing test case 9/15...\n",
      "Processing test case 10/15...\n",
      "Processing test case 11/15...\n",
      "Processing test case 12/15...\n",
      "Processing test case 13/15...\n",
      "Processing test case 14/15...\n",
      "Processing test case 15/15...\n",
      "Detailed test results saved to test_cases_report\\test_results_detailed.csv\n",
      "\n",
      "Test cases and report generated successfully in 'test_cases_report' directory\n",
      "Overall test accuracy: 93.33%\n",
      "All predictions saved to 'all_predictions_with_accuracy.csv'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import re\n",
    "import datetime as dt\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class GalaxyDataset(Dataset):\n",
    "    def __init__(self, root_dir, labeled=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.labeled = labeled\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "    def extract_label_from_filename(self, filename):\n",
    "        match = re.search(r'_(\\d+)$', os.path.splitext(filename)[0])\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return -1  \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = image.resize((128, 128))\n",
    "\n",
    "        image_array = np.array(image) / 255.0\n",
    "\n",
    "        image_tensor = torch.FloatTensor(image_array).permute(2, 0, 1)\n",
    "        \n",
    "        if self.labeled:\n",
    "\n",
    "            label = self.extract_label_from_filename(img_name)\n",
    "            return image_tensor, label, img_name\n",
    "        else:\n",
    "            return image_tensor, img_name\n",
    "\n",
    "class GalaxyCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(GalaxyCNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512 * 8 * 8, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def load_model(model_path, num_classes):\n",
    "    model = GalaxyCNN(num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def make_predictions(model, dataloader, labeled=True):\n",
    "    predictions = []\n",
    "    filenames = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader, desc=\"Making Predictions\"):\n",
    "            if labeled:\n",
    "                inputs, labels, img_names = data\n",
    "                true_labels.extend(labels.numpy())\n",
    "                filenames.extend(img_names)\n",
    "            else:\n",
    "                inputs, img_names = data\n",
    "                filenames.extend(img_names)\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            batch_preds = preds.cpu().numpy()\n",
    "            predictions.extend(batch_preds)\n",
    "    \n",
    "    if labeled:\n",
    "        return predictions, true_labels, filenames\n",
    "    else:\n",
    "        return predictions, filenames\n",
    "\n",
    "def print_validation_accuracy(true_labels, predictions):\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    correct = sum(1 for t, p in zip(true_labels, predictions) if t == p)\n",
    "    total = len(true_labels)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"VALIDATION ACCURACY: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Correct predictions: {correct}/{total}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def visualize_predictions(predictions, num_classes, title=\"Prediction Distribution\"):\n",
    "    counter = collections.Counter(predictions)\n",
    "\n",
    "    classes = list(range(num_classes))\n",
    "    counts = [counter.get(cls, 0) for cls in classes]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(classes, counts)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(title)\n",
    "    plt.xticks(classes)\n",
    "    plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "    print(f\"Distribution saved to '{title.lower().replace(' ', '_')}.png'\")\n",
    "    print(f\"\\n{title} Summary:\")\n",
    "    for cls in classes:\n",
    "        print(f\"Class {cls}: {counter.get(cls, 0)} images ({counter.get(cls, 0)/len(predictions)*100:.2f}%)\")\n",
    "\n",
    "def evaluate_model(true_labels, predictions, num_classes, filenames=None):\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"\\nModel Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(true_labels, predictions, labels=range(num_classes), zero_division=0)\n",
    "    print(report)\n",
    "\n",
    "    cm = confusion_matrix(true_labels, predictions, labels=range(num_classes))\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(num_classes), yticklabels=range(num_classes))\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    print(\"Confusion matrix saved to 'confusion_matrix_final.png'\")\n",
    "\n",
    "    print(\"\\nPer-Class Accuracy:\")\n",
    "    for i in range(num_classes):\n",
    "        class_correct = cm[i, i]\n",
    "        class_total = np.sum(cm[i, :])\n",
    "        if class_total > 0:\n",
    "            class_accuracy = class_correct / class_total\n",
    "            print(f\"Class {i}: {class_accuracy * 100:.2f}% ({class_correct}/{class_total})\")\n",
    "        else:\n",
    "            print(f\"Class {i}: N/A (0 samples)\")\n",
    "\n",
    "    if filenames is not None:\n",
    "        incorrect_predictions = []\n",
    "        for idx, (true, pred, fname) in enumerate(zip(true_labels, predictions, filenames)):\n",
    "            if true != pred:\n",
    "                incorrect_predictions.append(f\"{fname}: Predicted {pred}, True {true}\")\n",
    "        \n",
    "        if incorrect_predictions:\n",
    "            with open(\"incorrect_predictions_final.txt\", \"w\") as f:\n",
    "                for line in incorrect_predictions:\n",
    "                    f.write(f\"{line}\\n\")\n",
    "            print(f\"\\nSaved {len(incorrect_predictions)} incorrect predictions to 'incorrect_predictions.txt'\")\n",
    "\n",
    "# New function for test cases\n",
    "def create_test_cases_section(model, dataset, num_examples=10, save_dir=\"test_cases\"):\n",
    "    \"\"\"\n",
    "    Creates a test cases section with example images, predictions, and metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        dataset: Dataset to sample from\n",
    "        num_examples: Number of examples to include\n",
    "        save_dir: Directory to save test case images and results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GENERATING TEST CASES AND RESULTS SECTION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Sample random indices\n",
    "    indices = random.sample(range(len(dataset)), min(num_examples, len(dataset)))\n",
    "    \n",
    "    # Initialize lists to store results\n",
    "    test_results = []\n",
    "    \n",
    "    # Custom colormap for heatmap visualization\n",
    "    colors = [(0.6, 0.1, 0.1), (1.0, 1.0, 0.2), (0.1, 0.5, 0.1)]  # Red -> Yellow -> Green\n",
    "    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors, N=100)\n",
    "    \n",
    "    # Create the test report file\n",
    "    with open(os.path.join(save_dir, \"test_report.md\"), \"w\", encoding=\"utf-8\") as report_file:\n",
    "        report_file.write(\"# Galaxy Classification Test Cases and Results\\n\\n\")\n",
    "        report_file.write(f\"Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        report_file.write(\"## Individual Test Cases\\n\\n\")\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            print(f\"Processing test case {i+1}/{len(indices)}...\")\n",
    "            \n",
    "            # Get the image and true label\n",
    "            image_tensor, true_label, filename = dataset[idx]\n",
    "            \n",
    "            # Make prediction\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                input_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "                output = model(input_tensor)\n",
    "                probabilities = torch.nn.functional.softmax(output, dim=1)[0]\n",
    "                predicted_label = torch.argmax(output, dim=1).item()\n",
    "            \n",
    "            # Calculate accuracy for this example (0 or 1)\n",
    "            accuracy = 1.0 if predicted_label == true_label else 0.0\n",
    "            \n",
    "            # Store results\n",
    "            test_results.append({\n",
    "                \"index\": idx,\n",
    "                \"filename\": filename,\n",
    "                \"true_label\": true_label,\n",
    "                \"predicted_label\": predicted_label,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"confidence\": probabilities[predicted_label].item(),\n",
    "                \"all_probabilities\": probabilities.cpu().numpy()\n",
    "            })\n",
    "            \n",
    "            # Plot and save the image with prediction info\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            \n",
    "            # Original image\n",
    "            plt.subplot(2, 1, 1)\n",
    "            img = image_tensor.permute(1, 2, 0).numpy()\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Galaxy Image: {filename}\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Probability distribution\n",
    "            plt.subplot(2, 1, 2)\n",
    "            probs = probabilities.cpu().numpy()\n",
    "            bar_colors = [cmap(0.2) if i != predicted_label else cmap(0.8) for i in range(len(probs))]\n",
    "            bars = plt.bar(range(len(probs)), probs, color=bar_colors)\n",
    "            \n",
    "            # Highlight true label\n",
    "            if true_label < len(probs):\n",
    "                bars[true_label].set_color('blue')\n",
    "                bars[true_label].set_edgecolor('black')\n",
    "                bars[true_label].set_linewidth(2)\n",
    "            \n",
    "            plt.xticks(range(len(probs)))\n",
    "            plt.xlabel(\"Galaxy Class\")\n",
    "            plt.ylabel(\"Probability\")\n",
    "            plt.title(f\"Class Probabilities (Predicted: {predicted_label}, True: {true_label})\")\n",
    "            \n",
    "            # Save the figure\n",
    "            test_case_img_path = os.path.join(save_dir, f\"test_case_{i+1}.png\") \n",
    "            plt.savefig(test_case_img_path)\n",
    "            plt.close()\n",
    "            \n",
    "            # Write to report - using ASCII instead of Unicode symbols\n",
    "            report_file.write(f\"### Test Case {i+1}: {filename}\\n\\n\")\n",
    "            report_file.write(f\"![Test Case {i+1}]({os.path.basename(test_case_img_path)})\\n\\n\")\n",
    "            report_file.write(f\"- **Filename:** {filename}\\n\")\n",
    "            report_file.write(f\"- **True Class:** {true_label}\\n\")\n",
    "            report_file.write(f\"- **Predicted Class:** {predicted_label}\\n\")\n",
    "            # Using ASCII symbols instead of Unicode\n",
    "            report_file.write(f\"- **Correct:** {'YES' if accuracy == 1.0 else 'NO'}\\n\")\n",
    "            report_file.write(f\"- **Confidence:** {probabilities[predicted_label].item()*100:.2f}%\\n\\n\")\n",
    "            report_file.write(\"| Class | Probability |\\n\")\n",
    "            report_file.write(\"|-------|-------------|\\n\")\n",
    "            for class_idx, prob in enumerate(probabilities.cpu().numpy()):\n",
    "                report_file.write(f\"| {class_idx} | {prob*100:.2f}% |\\n\")\n",
    "            report_file.write(\"\\n---\\n\\n\")\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        correct_count = sum(1 for result in test_results if result[\"accuracy\"] == 1.0)\n",
    "        overall_accuracy = correct_count / len(test_results) * 100\n",
    "        avg_confidence = sum(result[\"confidence\"] for result in test_results) / len(test_results) * 100\n",
    "        \n",
    "        report_file.write(\"## Summary of Test Results\\n\\n\")\n",
    "        report_file.write(f\"- **Total Test Cases:** {len(test_results)}\\n\")\n",
    "        report_file.write(f\"- **Correctly Classified:** {correct_count}\\n\")\n",
    "        report_file.write(f\"- **Test Accuracy:** {overall_accuracy:.2f}%\\n\")\n",
    "        report_file.write(f\"- **Average Confidence:** {avg_confidence:.2f}%\\n\\n\")\n",
    "        \n",
    "        # Create a summary table\n",
    "        report_file.write(\"## Test Cases Overview\\n\\n\")\n",
    "        report_file.write(\"| # | Image | True Class | Predicted | Correct | Confidence |\\n\")\n",
    "        report_file.write(\"|---|-------|------------|-----------|---------|------------|\\n\")\n",
    "        \n",
    "        for i, result in enumerate(test_results):\n",
    "            img_path = f\"test_case_{i+1}.png\"\n",
    "            # Using plain text instead of Unicode symbols\n",
    "            correct_symbol = \"YES\" if result[\"accuracy\"] == 1.0 else \"NO\"\n",
    "            report_file.write(f\"| {i+1} | [{result['filename']}]({img_path}) | {result['true_label']} | {result['predicted_label']} | {correct_symbol} | {result['confidence']*100:.2f}% |\\n\")\n",
    "    \n",
    "    # Save detailed results to CSV\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Create detailed dataframe\n",
    "        df_detailed = pd.DataFrame(test_results)\n",
    "        \n",
    "        # Expand probabilities into separate columns\n",
    "        prob_cols = pd.DataFrame([{f'prob_class_{i}': probs[i] for i in range(len(probs))} \n",
    "                                for probs in df_detailed['all_probabilities']])\n",
    "        \n",
    "        # Remove the original probabilities column and merge with expanded probabilities\n",
    "        df_detailed = df_detailed.drop('all_probabilities', axis=1)\n",
    "        df_detailed = pd.concat([df_detailed, prob_cols], axis=1)\n",
    "        \n",
    "        # Save to CSV\n",
    "        df_detailed.to_csv(os.path.join(save_dir, \"test_results_detailed.csv\"), index=False)\n",
    "        print(f\"Detailed test results saved to {os.path.join(save_dir, 'test_results_detailed.csv')}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"pandas not installed, skipping detailed CSV export\")\n",
    "    \n",
    "    print(f\"\\nTest cases and report generated successfully in '{save_dir}' directory\")\n",
    "    print(f\"Overall test accuracy: {overall_accuracy:.2f}%\")\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = \"C:\\\\Users\\\\Aseem\\\\Desktop\\\\BE Project\\\\Decals_data_images\" \n",
    "    if not os.path.exists(data_dir):\n",
    "        raise FileNotFoundError(f\"Directory {data_dir} does not exist!\")\n",
    "\n",
    "    model_path = \"best_model.pth\"  \n",
    "    num_classes = 10  \n",
    "    validation_split = 0.2  \n",
    "    use_validation = True  \n",
    "\n",
    "    print(\"Setting up dataset with labels extracted from filenames\")\n",
    "    full_dataset = GalaxyDataset(root_dir=data_dir, labeled=True)\n",
    "\n",
    "    for i in range(min(10, len(full_dataset))):\n",
    "        _, label, filename = full_dataset[i]\n",
    "        print(f\"Sample {i}: Filename = {filename}, Extracted Label = {label}\")\n",
    "    \n",
    "    if use_validation:\n",
    "        dataset_size = len(full_dataset)\n",
    "        val_size = int(validation_split * dataset_size)\n",
    "        train_size = dataset_size - val_size\n",
    "        \n",
    "        train_dataset, val_dataset = random_split(\n",
    "            full_dataset, [train_size, val_size], \n",
    "            generator=torch.Generator().manual_seed(42) \n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset split into {train_size} training and {val_size} validation samples\")\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "        try:\n",
    "            model = load_model(model_path, num_classes)\n",
    "            print(f\"Model loaded successfully from {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            exit(1)\n",
    "\n",
    "        print(\"\\nEvaluating model on training set...\")\n",
    "        train_predictions, train_true_labels, train_filenames = make_predictions(model, train_loader, labeled=True)\n",
    "        print_validation_accuracy(train_true_labels, train_predictions) \n",
    "        evaluate_model(train_true_labels, train_predictions, num_classes, train_filenames)\n",
    "        \n",
    "        print(\"\\nEvaluating model on validation set...\")\n",
    "        val_predictions, val_true_labels, val_filenames = make_predictions(model, val_loader, labeled=True)\n",
    "\n",
    "        val_accuracy = print_validation_accuracy(val_true_labels, val_predictions)\n",
    "\n",
    "        with open(\"validation_accuracy.txt\", \"w\") as f:\n",
    "            f.write(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\\n\")\n",
    "            f.write(f\"Correct: {sum(1 for t, p in zip(val_true_labels, val_predictions) if t == p)}\\n\")\n",
    "            f.write(f\"Total: {len(val_true_labels)}\\n\")\n",
    "            f.write(f\"Date: {dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "        evaluate_model(val_true_labels, val_predictions, num_classes, val_filenames)\n",
    "\n",
    "        visualize_predictions(train_predictions, num_classes, \"Training Set Predictions\")\n",
    "        visualize_predictions(train_true_labels, num_classes, \"Training Set True Labels\")\n",
    "        visualize_predictions(val_predictions, num_classes, \"Validation Set Predictions\")\n",
    "        visualize_predictions(val_true_labels, num_classes, \"Validation Set True Labels\")\n",
    "        \n",
    "        # Generate test cases from validation set\n",
    "        print(\"\\nGenerating test cases from validation set...\")\n",
    "        # Create a custom dataset from validation indices for better control\n",
    "        val_indices = val_dataset.indices\n",
    "        test_cases_dataset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "        \n",
    "        # Generate test cases (adjust num_examples as needed)\n",
    "        test_results = create_test_cases_section(model, test_cases_dataset, num_examples=15, \n",
    "                                                save_dir=\"test_cases_report\")\n",
    "\n",
    "        all_predictions = train_predictions + val_predictions\n",
    "        all_true_labels = train_true_labels + val_true_labels\n",
    "        all_filenames = train_filenames + val_filenames\n",
    "        \n",
    "        try:\n",
    "            import pandas as pd\n",
    "            results_df = pd.DataFrame({\n",
    "                \"filename\": all_filenames,\n",
    "                \"true_class\": all_true_labels,\n",
    "                \"predicted_class\": all_predictions,\n",
    "                \"correct\": [p == t for p, t in zip(all_predictions, all_true_labels)]\n",
    "            })\n",
    "            results_df.to_csv(\"all_predictions_with_accuracy_final.csv\", index=False)\n",
    "            print(f\"All predictions saved to 'all_predictions_with_accuracy.csv'\")\n",
    "        except ImportError:\n",
    "            print(\"pandas not installed, skipping CSV export\")\n",
    "        \n",
    "    else:\n",
    "        dataloader = DataLoader(full_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        print(f\"Found {len(full_dataset)} images for classification\")\n",
    "        try:\n",
    "            model = load_model(model_path, num_classes)\n",
    "            print(f\"Model loaded successfully from {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            exit(1)\n",
    "\n",
    "        predictions, true_labels, filenames = make_predictions(model, dataloader, labeled=True)\n",
    "\n",
    "        print_validation_accuracy(true_labels, predictions)\n",
    "           \n",
    "        evaluate_model(true_labels, predictions, num_classes, filenames)\n",
    "        visualize_predictions(predictions, num_classes, \"Model Predictions\")\n",
    "        visualize_predictions(true_labels, num_classes, \"True Labels\")\n",
    "        \n",
    "        # Generate test cases from the full dataset\n",
    "        print(\"\\nGenerating test cases...\")\n",
    "        test_results = create_test_cases_section(model, full_dataset, num_examples=15, \n",
    "                                                save_dir=\"test_cases_report\")\n",
    "\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            results_df = pd.DataFrame({\n",
    "                \"filename\": filenames,\n",
    "                \"true_class\": true_labels,\n",
    "                \"predicted_class\": predictions,\n",
    "                \"correct\": [p == t for p, t in zip(predictions, true_labels)]\n",
    "            })\n",
    "            results_df.to_csv(\"predictions_with_accuracy.csv\", index=False)\n",
    "            print(f\"Predictions saved to 'predictions_with_accuracy.csv'\")\n",
    "        except ImportError:\n",
    "            print(\"pandas not installed, skipping CSV export\")\n",
    "\n",
    "import datetime"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
